---
title: "Project 3: stat302package Tutorial"
author: "Ruofeng Tang"
output: html_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, message = FALSE}
library(tidyverse)
# import data
my_gapminder <- read.csv("../Data/my_gapminder.csv")
my_penguins <- read.csv("../Data/my_penguins.csv")
# change my_penguins data structure to a tibble for my_rf_cv to work
my_penguins <- as_tibble(my_penguins)
# import function my_rf_cv
source("../Code/my_rf_cv.R")
```

## 5.Tutorial for `my_rf_cv`

This function implements random forest algorithm and k-fold cross validation together on given data. It takes the input data frame and the number of folds. It then returns a numeric with the cross validation error.

### Examples

Using `my_penguins` data, predict `body_mass_g` using covariates `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm` with k = 2, 5, 10. For each k, run the function 30 times and stores all MSE errors. 

```{r, message = FALSE}
library(randomForest)
# filter useful columns in my penguins dataset
dat2 <- na.omit(my_penguins[, c("body_mass_g", "bill_length_mm", 
                                "bill_depth_mm", "flipper_length_mm")])
# creates a matrix to store all MSE errors
results_rf <- matrix(NA, 30, 3)
# store k values wanted
count <- c(2, 5, 10)
# iterate through k = 2, 5, 10
for (i in 1 : 3) {
  for (j in 1 : 30) {
    # store MSE error for required k
    results_rf[j, i] <- my_rf_cv(dat2, count[i])
  }
}
```

Use `ggplot2` with 3 boxplots to display these data:

```{r, message = FALSE}
library(ggplot2)
# plot when k = 2
g2 <- ggplot() + 
  geom_boxplot(aes(y = results_rf[, 1])) + 
  scale_x_discrete( ) +
  labs(x = "k = 2", y = "MSE Errors", title = "30 MSE Errors when k = 2")
g2

# plot when k = 5
g5 <- ggplot() + 
  geom_boxplot(aes(y = results_rf[, 2])) + 
  scale_x_discrete( ) +
  labs(x = "k = 5", y = "MSE Errors", title = "30 MSE Errors when k = 5")
g5

# plot when k = 10
g10 <- ggplot() + 
  geom_boxplot(aes(y = results_rf[, 3])) + 
  scale_x_discrete( ) +
  labs(x = "k = 10", y = "MSE Errors", title = "30 MSE Errors when k = 10")
g10
```

Notice that as k increases, the MSE Errors generally increase significantly. 

Use a table to display the average CV estimate and the standard deviation of the CV estimates across k:

```{r, message = FALSE}
library(kableExtra)
# creates an empty data frame to store values
results_table <- data.frame(matrix(NA, 3, 2))
rownames(results_table) = c("k = 2", "k = 5", "k = 10")
colnames(results_table) = c("CV estimate", "Standard Deviation")
# store average CV estimates and standard deviations across k
for (i in 1 : 3) {
  results_table[i, 1] <- mean(results_rf[, i])
  results_table[i, 2] <- sd(results_rf[, i])
}
# presents the table
kable_styling(kable(results_table))
```

As k increases, the average CV estimate and the standard deviation increases significantly. From the boxplots, we can also see that all MSE errors increase as k increases. As we divide the dataset into more folds, each fold varies more, resulting in increasing errors. When we have only 2 folds, k = 2, we are just using two parts of the dataset to train each other; when we have 10 folds, k = 10, we are dealing with much more complicated training-testing situations, since we use 9 folds to predict 1 fold for 10 times.

### Save Results

```{r}
# save figures
ggsave("g2.pdf", g2, path = "../Output/Figures")
ggsave("g5.pdf", g5, path = "../Output/Figures")
ggsave("g10.pdf", g10, path = "../Output/Figures")

# save the table
saveRDS(results_table, file = "../Output/Results/table.rds")

# save simulation results
write.csv(results_rf, file = "../Output/Results/simulations.csv")
```
